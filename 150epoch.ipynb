{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "150epoch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMn6cST0oyrmD2qEHMWTDpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainiba/The-Collective-Mind/blob/main/150epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_dQlQ25B4fG"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv1B0RgVCEUW",
        "outputId": "dfc7ece6-d9b2-4d90-a0bf-73031b78a5a0"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open('input1.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "t\n",
            " \n",
            "s\n",
            "h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-qAHbyCJ6J"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfEa1fCjCTtr"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2f9hvKJCbne",
        "outputId": "624edcff-988d-46c8-f446-ba771b808172"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 98) # (batch_size, sequence_length, vocab_size)\n",
            "Prediction shape:  (64, 100, 98)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.586042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpCD_-v-Cksm"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxt9bUMxCnDs"
      },
      "source": [
        "EPOCHS = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWU-SLyICqsE",
        "outputId": "5a084ac8-715e-4e6d-9430-0af12dab9293"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 3.5983\n",
            "Epoch 2/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 2.6119\n",
            "Epoch 3/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 2.4046\n",
            "Epoch 4/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 2.2935\n",
            "Epoch 5/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 2.1837\n",
            "Epoch 6/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 2.0735\n",
            "Epoch 7/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 1.9642\n",
            "Epoch 8/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 1.8575\n",
            "Epoch 9/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 1.7597\n",
            "Epoch 10/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 1.6697\n",
            "Epoch 11/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 1.5841\n",
            "Epoch 12/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 1.5141\n",
            "Epoch 13/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 1.4487\n",
            "Epoch 14/150\n",
            "40/40 [==============================] - 191s 5s/step - loss: 1.3904\n",
            "Epoch 15/150\n",
            "40/40 [==============================] - 190s 5s/step - loss: 1.3367\n",
            "Epoch 16/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 1.2877\n",
            "Epoch 17/150\n",
            "40/40 [==============================] - 191s 5s/step - loss: 1.2358\n",
            "Epoch 18/150\n",
            "40/40 [==============================] - 191s 5s/step - loss: 1.1858\n",
            "Epoch 19/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 1.1383\n",
            "Epoch 20/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 1.0901\n",
            "Epoch 21/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 1.0384\n",
            "Epoch 22/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.9893\n",
            "Epoch 23/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.9389\n",
            "Epoch 24/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 0.8885\n",
            "Epoch 25/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 0.8335\n",
            "Epoch 26/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.7815\n",
            "Epoch 27/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 0.7294\n",
            "Epoch 28/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.6746\n",
            "Epoch 29/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.6280\n",
            "Epoch 30/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.5830\n",
            "Epoch 31/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.5339\n",
            "Epoch 32/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.4974\n",
            "Epoch 33/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.4630\n",
            "Epoch 34/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.4286\n",
            "Epoch 35/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.4008\n",
            "Epoch 36/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.3755\n",
            "Epoch 37/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.3559\n",
            "Epoch 38/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.3366\n",
            "Epoch 39/150\n",
            "40/40 [==============================] - 191s 5s/step - loss: 0.3232\n",
            "Epoch 40/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.3083\n",
            "Epoch 41/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.2959\n",
            "Epoch 42/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2875\n",
            "Epoch 43/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2771\n",
            "Epoch 44/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.2712\n",
            "Epoch 45/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2668\n",
            "Epoch 46/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.2619\n",
            "Epoch 47/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.2547\n",
            "Epoch 48/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.2517\n",
            "Epoch 49/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.2479\n",
            "Epoch 50/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2429\n",
            "Epoch 51/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2396\n",
            "Epoch 52/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.2360\n",
            "Epoch 53/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.2341\n",
            "Epoch 54/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.2296\n",
            "Epoch 55/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2278\n",
            "Epoch 56/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.2252\n",
            "Epoch 57/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.2244\n",
            "Epoch 58/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.2205\n",
            "Epoch 59/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2211\n",
            "Epoch 60/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2200\n",
            "Epoch 61/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.2161\n",
            "Epoch 62/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.2130\n",
            "Epoch 63/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.2119\n",
            "Epoch 64/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2129\n",
            "Epoch 65/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.2119\n",
            "Epoch 66/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.2085\n",
            "Epoch 67/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.2078\n",
            "Epoch 68/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.2082\n",
            "Epoch 69/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.2056\n",
            "Epoch 70/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.2051\n",
            "Epoch 71/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.2039\n",
            "Epoch 72/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 0.2035\n",
            "Epoch 73/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.2040\n",
            "Epoch 74/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.2010\n",
            "Epoch 75/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.1992\n",
            "Epoch 76/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1976\n",
            "Epoch 77/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 0.1988\n",
            "Epoch 78/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1979\n",
            "Epoch 79/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1960\n",
            "Epoch 80/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1982\n",
            "Epoch 81/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1955\n",
            "Epoch 82/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1955\n",
            "Epoch 83/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1948\n",
            "Epoch 84/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1924\n",
            "Epoch 85/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1932\n",
            "Epoch 86/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.1915\n",
            "Epoch 87/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1919\n",
            "Epoch 88/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1910\n",
            "Epoch 89/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1930\n",
            "Epoch 90/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1918\n",
            "Epoch 91/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1912\n",
            "Epoch 92/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1936\n",
            "Epoch 93/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.1911\n",
            "Epoch 94/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1896\n",
            "Epoch 95/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1887\n",
            "Epoch 96/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1896\n",
            "Epoch 97/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.1894\n",
            "Epoch 98/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1910\n",
            "Epoch 99/150\n",
            "40/40 [==============================] - 202s 5s/step - loss: 0.1879\n",
            "Epoch 100/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1899\n",
            "Epoch 101/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1898\n",
            "Epoch 102/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1886\n",
            "Epoch 103/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1897\n",
            "Epoch 104/150\n",
            "40/40 [==============================] - 200s 5s/step - loss: 0.1881\n",
            "Epoch 105/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.1882\n",
            "Epoch 106/150\n",
            "40/40 [==============================] - 200s 5s/step - loss: 0.1893\n",
            "Epoch 107/150\n",
            "40/40 [==============================] - 200s 5s/step - loss: 0.1860\n",
            "Epoch 108/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1855\n",
            "Epoch 109/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1857\n",
            "Epoch 110/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1876\n",
            "Epoch 111/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1878\n",
            "Epoch 112/150\n",
            "40/40 [==============================] - 200s 5s/step - loss: 0.1857\n",
            "Epoch 113/150\n",
            "40/40 [==============================] - 201s 5s/step - loss: 0.1862\n",
            "Epoch 114/150\n",
            "40/40 [==============================] - 205s 5s/step - loss: 0.1859\n",
            "Epoch 115/150\n",
            "40/40 [==============================] - 203s 5s/step - loss: 0.1868\n",
            "Epoch 116/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1854\n",
            "Epoch 117/150\n",
            "40/40 [==============================] - 200s 5s/step - loss: 0.1858\n",
            "Epoch 118/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1829\n",
            "Epoch 119/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1865\n",
            "Epoch 120/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1862\n",
            "Epoch 121/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1867\n",
            "Epoch 122/150\n",
            "40/40 [==============================] - 192s 5s/step - loss: 0.1864\n",
            "Epoch 123/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1863\n",
            "Epoch 124/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1859\n",
            "Epoch 125/150\n",
            "40/40 [==============================] - 194s 5s/step - loss: 0.1846\n",
            "Epoch 126/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1883\n",
            "Epoch 127/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1853\n",
            "Epoch 128/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1827\n",
            "Epoch 129/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1834\n",
            "Epoch 130/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1826\n",
            "Epoch 131/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1828\n",
            "Epoch 132/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1832\n",
            "Epoch 133/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1854\n",
            "Epoch 134/150\n",
            "40/40 [==============================] - 202s 5s/step - loss: 0.1817\n",
            "Epoch 135/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.1845\n",
            "Epoch 136/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1833\n",
            "Epoch 137/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1847\n",
            "Epoch 138/150\n",
            "40/40 [==============================] - 201s 5s/step - loss: 0.1859\n",
            "Epoch 139/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1839\n",
            "Epoch 140/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1840\n",
            "Epoch 141/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1857\n",
            "Epoch 142/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.1838\n",
            "Epoch 143/150\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.1844\n",
            "Epoch 144/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1863\n",
            "Epoch 145/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1870\n",
            "Epoch 146/150\n",
            "40/40 [==============================] - 199s 5s/step - loss: 0.1864\n",
            "Epoch 147/150\n",
            "40/40 [==============================] - 198s 5s/step - loss: 0.1893\n",
            "Epoch 148/150\n",
            "40/40 [==============================] - 196s 5s/step - loss: 0.1905\n",
            "Epoch 149/150\n",
            "40/40 [==============================] - 197s 5s/step - loss: 0.1903\n",
            "Epoch 150/150\n",
            "40/40 [==============================] - 193s 5s/step - loss: 0.1911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMSPkjp8CsNa",
        "outputId": "e41b1c5f-3b90-4f0c-9825-83a8bcbb755a"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([64, None]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rJ2l02ZCwqO"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    # model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yYWxHKbCzAv",
        "outputId": "6b5e5405-28a0-408c-b4cb-8fe81908b06e"
      },
      "source": [
        "print(generate_text(model, start_string=u\"Art \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Art that could be rother. On the progress pathern suggests that the machine is in itself is formed in Synthese resulting forms might constitute the \"event.\" In addition, Maciunas's description suggests that the significance of the bring nd in hand.\n",
            "\n",
            "In closing, there is no telling where my style will time benntes. New labion must have an HSBroadband server, which would need to be different than the low speed, and how long would not be devotely 9  essence in a time thers a series of discrete instructions that were maripulated at the whim of the netr on alization, whereby the more them defle of the planet. (T) in the separation of aitiating system. This project was in close proximity to a few leaf environment DCN. These material soll that talking examples of botang oberal.\n",
            "\n",
            "Ancient running on the internet where open the audion de creation of massive man made hard with a Marxist interesting way to create a framework for our thoughts, a common scaffolding upon which we can short pieces are sug\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}