{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "condensetrial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrOL/cT248/N12kjlzV01i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainiba/DigitalMediaBot/blob/main/condensetrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUfnGpzs1yDH"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG0FiZXo19XB"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open('input1.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "vocab = sorted(set(text))\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqJHPXN72GXk"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHy9d9OS2S44",
        "outputId": "a1415c0c-0d78-444b-b1fe-7a60049b0ccf"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "t\n",
            " \n",
            "s\n",
            "h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y15aS0x2Yt5"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5fpUfRx2a96"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x85W_NcP2lHz",
        "outputId": "bebfcb49-d9ed-4533-b14a-08592f6865fb"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAnm96-22ree"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHlP2RAC2uMV"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PDszCt2v3a"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1sHLL2Y3Na7",
        "outputId": "8dcc77ff-7abe-4e45-f77f-f427645f7f7b"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 98) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts2geXWE3Pg3",
        "outputId": "914ef8c1-2595-4179-a826-9fa523e8ba8e"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22, 60, 33, 59, 24, 27, 12, 49, 55, 97, 45, 92, 49, 55, 35, 75, 21,\n",
              "       64, 27, 28, 64, 28, 20, 94, 10, 62, 70, 46, 37,  1, 36, 67, 25, 29,\n",
              "       82, 15, 25, 44, 79, 87, 73, 29, 46, 25, 32, 97, 22, 67, 41, 35, 46,\n",
              "       87, 59, 28, 55, 47, 41, 67, 67, 88, 54,  6, 88, 55, 95, 36, 12, 32,\n",
              "        6,  4, 63, 91, 42, 91, 28, 20,  5, 56, 44, 75, 25, 39, 52, 69, 77,\n",
              "       83, 28, 77, 81, 13, 39,  5, 45, 94, 30, 49, 92, 41, 83, 48])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQxFcD143UOY",
        "outputId": "7beae3ec-908d-4e29-f6a4-f0bdddb79066"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 98)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.586711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYWM1yv53W8j"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvsBbqRU3Y7-"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzUByDIkAfUd"
      },
      "source": [
        "EPOCHS = 60\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0aIPVak3IVF",
        "outputId": "b2223186-a7fe-48be-b728-66ea6ae98149"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 3.0787\n",
            "Epoch 2/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 2.5354\n",
            "Epoch 3/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 2.3713\n",
            "Epoch 4/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 2.2574\n",
            "Epoch 5/60\n",
            "40/40 [==============================] - 166s 4s/step - loss: 2.1391\n",
            "Epoch 6/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 2.0216\n",
            "Epoch 7/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 1.9135\n",
            "Epoch 8/60\n",
            "40/40 [==============================] - 164s 4s/step - loss: 1.8044\n",
            "Epoch 9/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 1.7079\n",
            "Epoch 10/60\n",
            "40/40 [==============================] - 169s 4s/step - loss: 1.6217\n",
            "Epoch 11/60\n",
            "40/40 [==============================] - 172s 4s/step - loss: 1.5428\n",
            "Epoch 12/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 1.4778\n",
            "Epoch 13/60\n",
            "40/40 [==============================] - 170s 4s/step - loss: 1.4177\n",
            "Epoch 14/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 1.3623\n",
            "Epoch 15/60\n",
            "40/40 [==============================] - 166s 4s/step - loss: 1.3114\n",
            "Epoch 16/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 1.2622\n",
            "Epoch 17/60\n",
            "40/40 [==============================] - 169s 4s/step - loss: 1.2103\n",
            "Epoch 18/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 1.1638\n",
            "Epoch 19/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 1.1155\n",
            "Epoch 20/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 1.0670\n",
            "Epoch 21/60\n",
            "40/40 [==============================] - 174s 4s/step - loss: 1.0180\n",
            "Epoch 22/60\n",
            "40/40 [==============================] - 175s 4s/step - loss: 0.9704\n",
            "Epoch 23/60\n",
            "40/40 [==============================] - 176s 4s/step - loss: 0.9174\n",
            "Epoch 24/60\n",
            "40/40 [==============================] - 174s 4s/step - loss: 0.8664\n",
            "Epoch 25/60\n",
            "40/40 [==============================] - 172s 4s/step - loss: 0.8180\n",
            "Epoch 26/60\n",
            "40/40 [==============================] - 170s 4s/step - loss: 0.7646\n",
            "Epoch 27/60\n",
            "40/40 [==============================] - 174s 4s/step - loss: 0.7106\n",
            "Epoch 28/60\n",
            "40/40 [==============================] - 170s 4s/step - loss: 0.6616\n",
            "Epoch 29/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.6121\n",
            "Epoch 30/60\n",
            "40/40 [==============================] - 169s 4s/step - loss: 0.5698\n",
            "Epoch 31/60\n",
            "40/40 [==============================] - 172s 4s/step - loss: 0.5222\n",
            "Epoch 32/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 0.4853\n",
            "Epoch 33/60\n",
            "40/40 [==============================] - 170s 4s/step - loss: 0.4517\n",
            "Epoch 34/60\n",
            "40/40 [==============================] - 173s 4s/step - loss: 0.4198\n",
            "Epoch 35/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 0.3944\n",
            "Epoch 36/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 0.3754\n",
            "Epoch 37/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.3506\n",
            "Epoch 38/60\n",
            "40/40 [==============================] - 169s 4s/step - loss: 0.3340\n",
            "Epoch 39/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.3206\n",
            "Epoch 40/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 0.3068\n",
            "Epoch 41/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.2933\n",
            "Epoch 42/60\n",
            "40/40 [==============================] - 166s 4s/step - loss: 0.2856\n",
            "Epoch 43/60\n",
            "40/40 [==============================] - 164s 4s/step - loss: 0.2773\n",
            "Epoch 44/60\n",
            "40/40 [==============================] - 164s 4s/step - loss: 0.2690\n",
            "Epoch 45/60\n",
            "40/40 [==============================] - 169s 4s/step - loss: 0.2621\n",
            "Epoch 46/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.2581\n",
            "Epoch 47/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 0.2532\n",
            "Epoch 48/60\n",
            "40/40 [==============================] - 166s 4s/step - loss: 0.2486\n",
            "Epoch 49/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 0.2449\n",
            "Epoch 50/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 0.2420\n",
            "Epoch 51/60\n",
            "40/40 [==============================] - 170s 4s/step - loss: 0.2356\n",
            "Epoch 52/60\n",
            "40/40 [==============================] - 169s 4s/step - loss: 0.2355\n",
            "Epoch 53/60\n",
            "40/40 [==============================] - 165s 4s/step - loss: 0.2338\n",
            "Epoch 54/60\n",
            "40/40 [==============================] - 166s 4s/step - loss: 0.2294\n",
            "Epoch 55/60\n",
            "40/40 [==============================] - 166s 4s/step - loss: 0.2273\n",
            "Epoch 56/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.2244\n",
            "Epoch 57/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.2202\n",
            "Epoch 58/60\n",
            "40/40 [==============================] - 168s 4s/step - loss: 0.2192\n",
            "Epoch 59/60\n",
            "40/40 [==============================] - 173s 4s/step - loss: 0.2158\n",
            "Epoch 60/60\n",
            "40/40 [==============================] - 167s 4s/step - loss: 0.2164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bhymQmYN3kmR",
        "outputId": "a0c6a3e3-4520-4189-a7db-a267bfaf188c"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_60'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd8oC-Ur_NXS"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([64, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5tujFv33niq"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.7\n",
        "\n",
        "    # Here batch size == 1\n",
        "    # model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VlIFei43oZg",
        "outputId": "4c464c16-b7ac-4cd6-9566-3b34739fe28d"
      },
      "source": [
        "print(generate_text(model, start_string=u\"Technology \"))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Technology in and of its inction is made between reading, watching, playing, and  experiencing  digital fiction is rarer than access to print picnic table I described would not be designed, a biology produce fantastical and elaborate hallucinations of ornamental patterns when gazing.  The reproduction and redistribution of digital control and design of this potround us, back to our needs to explore new worlds with other people, born in a plant has the instructions or algorithms are required for the libraries6 of the future as well as purveyors of creativity is altered the function of the event score is \"a ciece. I may established ideas of space, form, function, and use in order to reflection place in San Jose, California. A symposium kicked off the Biennial. The to resulted a string of eights. Eight is a guide, and cutting the pattern s pieces out by hand with die-cut leaf tissue s disembodied cells contain such total potentiality, but imagining how this technological predisposition is meaningles\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}